\section{Introduction}

Avec les GPS, les radars de recul, les assistants de créneaux et autres gadgets, les aides à la conduite deviennent de plus en plus intelligentes.
Devant le nombre de capteurs et de calculateurs embarqués on est en droit de se demander si le conducteur est encore nécessaire.

Il existe déjà des projets comme la voiture Google\footnote{•} qui roule de façon autonome sur certaines les routes américaines. Mais ces voitures nécessite un nombre très important de capteurs avec une précision très importante.
On y retrouve un GPS précis au centimètre, des télémètres lasers plans ou 3D, ainsi que des accéléromètres multi-directionnels.

Ces capteurs ont un coût important et ne sont pas à la porté des particuliers.
C'est pourquoi nous travaillons sur des véhicules autonome utilisant la vision, le prix d'une caméra étant largement inférieur à celui d'un télémètre laser 3D.

Cependant, les algorithmes en développement ne sont pas encore adaptés à des véhicules roulant à haute vitesse.
En attendant que le matériel et les programmes le permettent, nous allons utiliser un véhicule autonome roulant à basse vitesse, un VipaLab (voir figure~\ref{fig:vipalab}).

\fig{images/videH.jpg}{Un VipaLab, véhicule de la société \warning{X}}{fig:vipalab}

Le but final est d'avoir une flotte de véhicules autonomes, en libre service, dans le centre ville de Clermont-Ferrand.
A partir d'un point initial, le robot devra \^etre capable de planifier son trajet jusqu'à la destination souhaitée par l'utilise et de l'exécuter de manière totalement autonome mais sécurisé.
Effectivement le robot pourra \^etre amené à rouler dans des zones piétonnes (respect des piétons, évitement de collisions) mais également sur certaines portions de route (respect du code de la route).

Dans un environnement aussi contraint qu'une zone piétonne, le robot doit pouvoir observer ce qu'il se passe tout autour de lui.
L'utilisation de caméras perspectives avant et arrière ne sera pas obligatoirement suffisant dans certains cas.
C'est pourquoi nous avons souhaité travailler avec des caméras omnidirectionnelles (caméra catadioptrique, voir figure~\ref{fig:camcata}).

\smallfig{0.4}{images/videV}{Caméra omnidirectionnelle composée d'une caméra affine et d'un miroir parrabolique}{fig:camcata}

Notre flotte de véhicule ne sera pas obligatoirement homogène, on y retrouvera différents robots (véhicules) et différents capteurs (caméras).
La différences entre les robots n'impactera pas sur notre travaille car la gestion de la trajectoire sera calculé par un programme tiers.
La différences entre les caméras aura, quand à elle, un impacte très important sur les travaux à réalisés.
Il existe de nombreux algorithme de SLAM\footnote{Simultaneous Localisation And Mapping -- Cartographie et localisation simultanée} utilisant la vision.
Cependant ils ne se concentrent que sur un type de caméra à la fois.

L'innovation demandé dans ce travail de recherche est de mettre en place de nouveaux algorithmes permettant la cartographie et la navigation pour une flotte de robots munies de caméras de type différents.
Pour parler une paire de caméras, composée d'une caméra perspective et d'une caméra omnidirectionnelle, nous utiliserons le terme de paire hybride, d'où la notion de vision hybride.
